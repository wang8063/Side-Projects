{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Language Translation\n",
    "<div align=\"center\">\n",
    "<font size=3><b>\n",
    "<br>Ruobing Wang\n",
    "<br>April 20, 2019\n",
    "<br></font></b></div>\n",
    "\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, I will use the Tensorflow to build a language translation model using sequence to sequence (seq2seq) or encoder-decoder. From the name we can see encoder-decoder includes two parts: Encoder and Decoder. The purpose is to translate English sentences to French sentence by using from WMT10 French- English corpus dataset which includes a source file (English) and a target file(French).  \n",
    "\n",
    "Natural Language Processing becomes more and more important for lives. Such like machine translation(for here) given a piece of text in one language, translate to another language. The ability to quickly and automatically translate anything will make huge profit in real world. With machine translation, you can sit in America and invest the France's market as long as you understand what happened in real time.\n",
    "\n",
    "This project includes: how to preprocess the dataset, how to define inputs, how to define encoder model, how to define decoder model, how to build the entire seq2seq model, how to calculate the loss and clip gradients, and how to train and get prediction.\n",
    "\n",
    "### References:\n",
    "\n",
    "https://github.com/angelmtenor/data-science-keras\n",
    "\n",
    "https://github.com/deep-diver/EN-FR-MLT-tensorflow\n",
    "\n",
    "https://github.com/udacity/deep-learning\n",
    "\n",
    "https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "[Why special tokens?](https://datascience.stackexchange.com/questions/26947/why-do-we-need-to-add-start-s-end-s-symbols-when-using-recurrent-neural-n)\n",
    "\n",
    "[Python enumerate](https://docs.python.org/3/library/functions.html#enumerate)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Data\n",
    "\n",
    "The original dataset is at\n",
    "\n",
    "(WMT10 French-English corpus(http://www.statmt.org/wmt10/training-giga-fren.tar)). \n",
    "\n",
    "Since I only use the laptop for this project, I chose a relatively small dataset. \n",
    "\n",
    "These small datasets store English(source file) and French(target file) sentences but there are also some other language in this data set. Maybe Russian. I manually delete these lines. And these two files exactly contains the same number of lines and mapping well which means the same line has the same meaning in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of the datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Brief Stats\n",
      "* number of unique words in English sample sentences: 227        [this is roughly measured/without any preprocessing]\n",
      "\n",
      "* English sentences\n",
      "\t- number of sentences: 137861\n",
      "\t- avg. number of words in a sentence: 13.225277634719028\n",
      "* French sentences\n",
      "\t- number of sentences: 137861 [data integrity check / should have the same number]\n",
      "\t- avg. number of words in a sentence: 14.226612312401622\n",
      "\n",
      "* Sample sentences range from 0 to 5\n",
      "[1-th] sentence\n",
      "\tEN: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\tFR: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "[2-th] sentence\n",
      "\tEN: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\tFR: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "[3-th] sentence\n",
      "\tEN: california is usually quiet during march , and it is usually hot in june .\n",
      "\tFR: california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n",
      "[4-th] sentence\n",
      "\tEN: the united states is sometimes mild during june , and it is cold in september .\n",
      "\tFR: les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "\n",
      "[5-th] sentence\n",
      "\tEN: your least liked fruit is the grape , but my least liked is the apple .\n",
      "\tFR: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print('Dataset Brief Stats')\n",
    "print('* number of unique words in English sample sentences: {}\\\n",
    "        [this is roughly measured/without any preprocessing]'.format(len(Counter(source_text.split()))))\n",
    "print()\n",
    "\n",
    "english_sentences = source_text.split('\\n')\n",
    "print('* English sentences')\n",
    "print('\\t- number of sentences: {}'.format(len(english_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in english_sentences])))\n",
    "\n",
    "french_sentences = target_text.split('\\n')\n",
    "print('* French sentences')\n",
    "print('\\t- number of sentences: {} [data integrity check / should have the same number]'.format(len(french_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in french_sentences])))\n",
    "print()\n",
    "\n",
    "sample_sentence_range = (0, 5)\n",
    "side_by_side_sentences = list(zip(english_sentences, french_sentences))[sample_sentence_range[0]:sample_sentence_range[1]]\n",
    "print('* Sample sentences range from {} to {}'.format(sample_sentence_range[0], sample_sentence_range[1]))\n",
    "\n",
    "for index, sentence in enumerate(side_by_side_sentences):\n",
    "    en_sent, fr_sent = sentence\n",
    "    print('[{}-th] sentence'.format(index+1))\n",
    "    print('\\tEN: {}'.format(en_sent))\n",
    "    print('\\tFR: {}'.format(fr_sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary, we can see the dataset has the exactly of the same number of sentences and match well(ith sentence in English has the same meaning with ith sentence in French)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are brief overview what steps will be done in this section\n",
    "\n",
    "- **create lookup tables** \n",
    "  - create two mapping tables \n",
    "      - (key, value) == (unique word string, its unique index)     - `(1)`\n",
    "      - (key, value) == (its unique index, unique word string)     - `(2)`\n",
    "      - `(1)` is used in the next step, and (2) is used later for prediction step\n",
    "      \n",
    "      \n",
    "- **text to word ids**\n",
    "  - convert each string word in the list of sentences to the index\n",
    "  - `(1)` is used for converting process\n",
    "  \n",
    "  \n",
    "- **save the pre-processed data**\n",
    "  - create two `(1)` mapping tables for English and French\n",
    "  - using the mapping tables, replace strings in the original source and target dataset with indicies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "\n",
    "### Create Lookup Tables\n",
    "\n",
    "As mentioned breifly, I am going to implement a function to create lookup tables. Since every models are mathmatically represented, the input and the output(prediction) should also be represented as numbers. That is why this step is necessary for NLP problem because human readable text is not machine readable. This function takes a list of sentences and returns two mapping tables (dictionary data type). Along with the list of sentences, there are special tokens, `<PAD>` for pading the sentence to get same length of both of the source sentence and the target sentence, `<EOS>` tell us we should we stop outputting the sentence , `<UNK>` for the words do not appear in the sentence, and `<GO>` to be added in the mapping tables for letting the sentence output to start. \n",
    "\n",
    "- (key, value) == (unique word string, its unique index)     - `(1)`\n",
    "- (key, value) == (its unique index, unique word string)     - `(2)`\n",
    "\n",
    "`(1)` will be used in the next step, `test to word ids`, to find a match between word and its index. `(2)` is not used in pre-processing step, but `(2)` will be used later. After making a prediction, the sequences of words in the output sentence will be represented as their indicies. The predicted output is machine readable but not human readable. That is why we need `(2)` to convert each indicies of words back into human readable words in string. \n",
    "\n",
    "<br/>\n",
    "<img src='https://i.ibb.co/BPjPwrt/lookup.png' alt='Drawing' width='70%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two tables:\n",
    "\n",
    "one is for vocabulary to its own ID\n",
    "\n",
    "the another is for its own ID to this vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Word Ids\n",
    "\n",
    "Two `(1)` lookup tables (one for English sentences and the another is for French) will be provided in `text_to_ids` functions as arguments. They will be used in the converting process for English(source) and French(target) respectively. This part is more like a programming part, so there are not much to mention. I will just go over few minor things to remember before jumping in.\n",
    "\n",
    "- original(raw) source & target datas contain a list of sentences\n",
    "  - they are represented as a string \n",
    "\n",
    "- the number of sentences are the same for English and French\n",
    " \n",
    "- by accessing each sentences, need to convert word into the corresponding index. For example, as the graph shows, we convert \"This is a short sentence\" to a list include numbers : [18,19,3,20,21]\n",
    "  - each word should be stored in a list\n",
    "  - this makes the resuling list as a 2-D array ( row: sentence, column: word index )\n",
    "  \n",
    "- for every target sentences, special token, `<EOS>` should be inserted at the end\n",
    "  - this token suggests when to stop creating a sequence\n",
    "  \n",
    "<br/>\n",
    "<img src='https://i.ibb.co/2Zn4kx1/conversion.png' alt='Drawing' width='100%'>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have two list \n",
    "\n",
    "one includes the id of the text of English and the other includes the id of the text of French."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Save Data\n",
    "\n",
    "Now, we have the functions: `create_lookup_tables`, `text_to_ids` and we will use these functions in this step.\n",
    "\n",
    "`create_lookup_tables`, `text_to_ids` are generalized functions. It can  be used for other languages too. In this particular project, the target languages are English and French, so those languages have to fed into `create_lookup_tables`, `text_to_ids` functions to generate pre-processed dataset for this project. Here is the steps to do it basically it is the summary of what we did before.\n",
    "\n",
    "- Load data(text) from the original file for English and French\n",
    "- Make them lower case letters\n",
    "- Create lookup tables for both English and French\n",
    "- Convert the original data into the list of sentences whose words are represented in index\n",
    "- Finally, in this step, we combine these steps together and save the preprocessed data to the external file (checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (English, French)\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    # to the lower case\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for English and French data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    " This project uses a small set of sentences. However, in general, NLP requires a huge amount of raw text data. It would take quite a long time to preprocess, so it is recommended to avoid whenever possible.(unFortunately shutdown). Also, saving the preprocessed data to the external file could speed up your job and let you focus more on building a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "Since the Recurrent Neural Networks is kind of heavy model to train, it is recommended to train the model in GPU environment. I use AWS to run this step since my laptop does not have a GPU. \n",
    "\n",
    "From this step, you can check your version of TensorFlow to avoid problems, mine is 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.13.1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the machine translation, I will build a model named sequence to sequence (seq2seq in short). \n",
    "\n",
    "Basically, it has two sub- models. Encoder and Decoder. In each of the sub- models, utilize RNN. For Encoder, it takes raw input text data and outputs a neural representation. And the output of Encoder is the input data for Decoder.\n",
    "\n",
    "Encoder makes an output encoded in neural representational form. Let's call it C. We do not know what it really is but decoder has the ability to look inside the C and create another different output data (for here, it is French)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a model, I will use the following steps. For overview:\n",
    "\n",
    "- define and process the input parameters\n",
    "    - for encoder model\n",
    "        - enc_dec_model_inputs\n",
    "    - for the decoder model \n",
    "        - enc_dec_model_inputs, process_decoder_input, decoding_layer\n",
    "- build encoder model \n",
    "    - encoding_layer\n",
    "- Build decoder model for training (decoding- training process)\n",
    "    - decoding_layer_train\n",
    "- Build decoder model for inference process \n",
    "    - decoding_layer_infer\n",
    "- Build the Seq2Seq model (connect encoder and decoder models)\n",
    "    - seq2seq_model\n",
    "- Train and estimate loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters\n",
    "\n",
    "enc_dec_model_inputs function creates and return parameters related to building model.\n",
    "\n",
    "Inputs placeholder will be (here) fed with English sentence data and the shape is [None, None] which is because the first None is the batch size that user can define and the second None is the lengths of sentences. Then maximum length of sentence is different from batch to batch so it cannot be set with an exact number. But we can set the lengths of every sentences to the maximum length for all the sentences in every batch. But remember to set the `<PAD>` in empty positions. We will deal with it later.\n",
    "\n",
    "targets place holder is similar to inputs placeholder except for target we will feed French sentence data.\n",
    "\n",
    "target_sequence_length placeholder represents the lengths of each sentences, since the shape is not fixed and the same number to the batch size. It is None for here. And we need a particular value as an argument of TrainerHelper to build decoder model later.\n",
    "\n",
    "max_target_len gets the maximum value out of lengths of all target sentences. Since we store the lengths of all the sentences in target_sequence_length parameter. We can use reduce_max which can computes the maximum of elements across dimensions of a tensor to get the maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.placeholder will take the dtype of the elements and shape and name as the arguments and it will insert a placeholder for a tensor that will be fed (we fed English or French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparam_inputs function creates and returns parameters (TF placeholders) related to hyper-parameters to the model. \n",
    "- lr_rate is learning rate\n",
    "- keep_prob is the keep probability for Dropouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "<br/>\n",
    "<img src=\"https://i.ibb.co/gPrMr27/go-insert.png\" style=\"width:600px;\"/>\n",
    "<div style=\"text-align:center;\">Fig 2. `<GO>` insertion</div>\n",
    "<br/>\n",
    "\n",
    "For Decoder, we need two different inout for training and predict purposes respectively. In training phase, the input is provided as target label, but still need to be embeded. On the inference phase, the output of each time step will be the input for the next time step. They also need to be embeded and embedding vector should be shared between two different phases.\n",
    "\n",
    "In order to start the translation in Decoder or preprocess the target label data for the training phase in other words, we need to add <GO> token in front of all target data to tell model you should start now. I use 3 functions from Tensorflow to solve this:\n",
    "    \n",
    "- TFstrided_slice(TF Tensor, Begin, End, Strides)\n",
    "    - extracts a stride slice of tensor, generalize python array indexing\n",
    "    - can split into multiple tensors with the striding window size from begin to end\n",
    "- TF fill (TF tensor, values)\n",
    "    - creates a tensor filled with scalar value\n",
    "- TF concat (list of TF tensor, tf fill and after slice)\n",
    "    - concatenates tensors along one dimension\n",
    "After process the target label data, we will embed it later when implementing decoding_layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocess the target label, we need to embed it later when we implement decoding_ layer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://i.ibb.co/8x4jpTC/fg3.png\" style=\"width:600px;\"/>\n",
    "<div style=\"text-align:center;\">Fig 3. Encoding model highlighted - Embedding/RNN layers</div>\n",
    "<br/>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding includes two parts: Embedding layer and RNN layer. Each word in a sentence will be represented with the number of features in the encoding_embedding_size. The reasons why we use tf.contrib.layers.embed_sequence are\n",
    "- Reduce the number of parameters in the network while preserving depth.\n",
    "- It allows for arbitrary input shapes, which helps the implementation be simple and flexible\n",
    "\n",
    "For RNN layers, we use LSTM. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). Multiple LSTM cells are stacked together after dropout technique is applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer:\n",
    "tf.contrib.layers.embed_sequence\n",
    "\n",
    "RNN layers:\n",
    "\n",
    "- tf.nn.rnn_cell.LSTMCell \n",
    "    - simply specifies how many internal units it has\n",
    "- tf.contrib.rnn.DropoutWrapper\n",
    "    - wraps a cell with keep probability value\n",
    "- tf.contrib.rnn.MultiRNNCell\n",
    "    - stacks multiple RNN (type) cells\n",
    "    - To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "\n",
    "- tf.nn.dynamic_rnn\n",
    "    - connect Embedding layer and RNN layers all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training process \n",
    "\n",
    "Decoding model can be thought of two separate processes, training and inference. It is not they have different architecture, but they share the same architecture and its parameters. It is that they have different strategy to feed the shared model.\n",
    "\n",
    "For this(training) and the next(inference) section, Fig 4 shows clearly shows what they are.\n",
    "\n",
    "<img src=\"https://i.ibb.co/XkrwRy4/decoder-shift.png\" style=\"width:700px;\"/>\n",
    "<div style=\"text-align:center;\">Fig 4. Decoder shifted inputs</div>\n",
    "<br/>\n",
    "\n",
    "From the graph, we can see, for the decoder, the output of the current time step will be the input of the next time step. Thus, rather than what we did in in encoding, in the encoding, we actually prepared dataset before running by using the function  TF contrib.layers.embed_sequence. We use dynamic embedding capability. Also, from the graph, we can see the training and inference processes share the same embedding parameter. In training, embed input should be delivered. In the prediction, only embedding parameters used in the training part should be delivered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do training part first, the function I used as following:\n",
    "\n",
    "- tf.contrib.seq2seq.TrainingHelper\n",
    "    - TrainingHelper is where we pass the embeded input.This is not a decoder model, just a helper instance. We need to pass it into BasicDecoder, the actual process of building the decoder model;\n",
    "\n",
    "- tf.contrib.seq2seq.BasicDecoder\n",
    "    - BasicDecoder builds the decoder model. It means it connects the RNN layer(s) on the decoder side.\n",
    "\n",
    "- tf.contrib.seq2seq.dynamic_decode\n",
    "    - dynamic_decode unrolls the decoder model so that actual prediction can be retrieved by BasicDecoder for each time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference process\n",
    "\n",
    "For inferencing, for example, we take the first output and plug it in the next word. And that is why we need to use GreedyEmbeddingHelper function here.\n",
    "\n",
    "- tf.contrib.seq2seq.GreedyEmbeddingHelper\n",
    "  - GreedyEmbeddingHelper dynamically takes the output of the current step and give it to the next time step's input. In order to embed the each input result dynamically, embedding parameter(just bunch of weight values) should be provided. Along with it, GreedyEmbeddingHelper asks to give the start_of_sequence_id for the same amount as the batch size and end_of_sequence_id. \n",
    "- tf.contrib.seq2seq.BasicDecoder\n",
    "  - same as described in the training process section\n",
    "- tf.contrib.seq2seq.dynamic_decode\n",
    "  - same as described in the training process section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer \n",
    "\n",
    "__Embed the target sequences__\n",
    "\n",
    "- TF contrib.layers.embed_sequence creates internal representation of embedding parameter, so we cannot look into or retrieve it. \n",
    "\n",
    "- Manually created embedding parameter is used for training phase to convert provided target data(sequence of sentence) by TF nn.embedding_lookup before the training is run. \n",
    "\n",
    "TF nn.embedding_lookup creates embedding parameters return the similar result to the TF contrib.layers.embed_sequence. For the inference process, dynamic whenever the output of the current time step is calculated via decoder, then embeded by the shared embedding parameter and become the input for the next step. We can just use the embedding parameter to plug in the the helper, then it will do the process.\n",
    "\n",
    "embedding_lookup function retrieves rows of the params tensor. Like how we use index with arrays in numpy. In short, it selects specified rows.\n",
    "\n",
    "__Construct the decoder RNN layer(s)__\n",
    "- As Fig 3 and Fig 4 show, the number of RNN layer in the decoder model has to be equal to the number of RNN layer(s) in the encoder model.\n",
    "\n",
    "Finally, we create an output layer to map the outputs of the decoder to the elements of our vocabulary and connect all layers to get probabilities of occurance of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Seq2Seq model\n",
    "Now, we can use the previously defined functions\n",
    "encoding_layer, process_decoder_input, and decoding_layer put together to build the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 300\n",
    "\n",
    "epochs = 13\n",
    "batch_size = 128\n",
    "\n",
    "#epochs = 6\n",
    "#batch_size = 256\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set after 300 each batch, it will appear once, \n",
    "Epochs means to let the entire dataset(English or French) pass forward and backward through the neural network of 13 times. \n",
    "Set Batch = 128 is let the total number of training example present in a single batch. When I test, I set the size = 256 which we can increase the speed.\n",
    "When we talk about the learning rate we need to talk about Gradient Descent. It is an algorithm to iterative optimization. Iteration to get the most optimal output. It has a parameter names learning rate. It can determine to what extent newly acquired information overrides old information. The lower the value is, the slower we travel along the downward slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "The word Graph here is not really graph. It likes a structure. \n",
    "\n",
    "Tensorflow(version 1) uses a dataflow graph to represent computations in terms of the dependencies between individual operations. This leads to a low- level programming model in which we can first define the dataflow graph, then create a TensorFlow seeion to run parts of the graph across a set of local and remote devices. It also includes operations and tensors.\n",
    "\n",
    "tf.Graph contains two relevant kinds of information:\n",
    "- Graph structure\n",
    "- Graph collections: for example tf.train.Optimizer\n",
    "\n",
    "`seq2seq_model` function creates the model. It defines how the feedforward and backpropagation should flow. The last step for this model to be trainable is deciding and applying what optimization algorithms to use. In this section, [TF contrib.seq2seq.sequence_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) is used to calculate the loss, then [TF train.AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) is applied to calculate the gradient descent on the loss. Let's go over eatch steps in the code cell below.\n",
    "\n",
    "__load data from the checkpoint__\n",
    "- (source_int_text, target_int_text) are the input data, and (source_vocab_to_int, target_vocab_to_int) is the dictionary to lookup the index number of each words.\n",
    "- max_target_sentence_length is the length of the longest sentence from the source input data. This will be used for GreedyEmbeddingHelper when building inference process in the decoder mode.\n",
    "\n",
    "__create inputs__\n",
    "- inputs (input_data, targets, target_sequence_length, max_target_sequence_length) from enc_dec_model_inputs function\n",
    "- inputs (lr, keep_prob) from hyperparam_inputs function\n",
    "\n",
    "__build seq2seq model__\n",
    "- build the model by seq2seq_model function. It will return train_logits(logits to calculate the loss) and inference_logits(logits from prediction).\n",
    "\n",
    "__cost function__\n",
    "- [TF contrib.seq2seq.sequence_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) is used. This loss function is just a weighted softmax cross entropy loss function, but it is particularly designed to be applied in time series model (RNN). \n",
    "\n",
    "So, what is softmax function. It takes an N- dimensional vector of real numbers and transforms it into a vector of real number in range (0,1)\n",
    "\n",
    "And what is Cross Entropy loss? Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution really is.\n",
    "\n",
    "__Optimizer__\n",
    "- TF train.AdamOptimizer is used, and this is where the learning rate should be specified. You can choose other algorithms as well, this is just a choice.\n",
    "\n",
    "__Gradient Clipping__\n",
    "- Since recurrent neural networks is notorious about vanishing/exploding gradient, gradient clipping technique is believed to improve the issues. \n",
    "- The concept is really easy. You decide thresholds to keep the gradient to be in a certain boundary. In this project, the range of the threshold is between -1 and 1.\n",
    "- Now, you need to apply this conceptual knowledge to the TensorFlow code. In breif, you get the gradient values from the optimizer manually by calling compute_gradients, then manipulate the gradient values with clip_by_value. Lastly, you need to put back the modified gradients into the optimizer by calling apply_gradients.\n",
    "\n",
    "<img src=\"https://i.ibb.co/DRDXyCx/gradient-clipping.png\" style=\"width:700px;\"/>\n",
    "<div style=\"text-align:center;\">Fig 4. Gradient Clipping</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Batches and Pad the source and target sequences\n",
    "<br/>\n",
    "<img src=\"https://i.ibb.co/wBZgXM6/pad-insert.png\" style=\"width:300px;\"/>\n",
    "<div style=\"text-align:center;\">Fig 5. Padding character in empty space of sentences shorter than the longest one in a batch</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "`get_accuracy`\n",
    "- compare the lengths of target(label) and logits(prediction)\n",
    "- add(pad) 0s at the end of the ones having the shorter length\n",
    "  - `[(0,0),(0,max_seq - target.shape[1])]` indicates the 2D array. The first (0,0) means no padding for the first dimension. The second (0, ...) means there is no pads in front of the second dimension but pads at the end. And pad as many times as until makes two entities to have the same shape (length)\n",
    "- finally, returns the average of where the target and logits have the same value (1) because they must have the same shape.\n",
    "\n",
    "[numpy pad function](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.pad.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  300/1077 - Train Accuracy: 0.3993, Validation Accuracy: 0.4876, Loss: 1.9738\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy: 0.4747, Validation Accuracy: 0.4911, Loss: 1.0928\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy: 0.5473, Validation Accuracy: 0.5987, Loss: 0.8654\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy: 0.6110, Validation Accuracy: 0.6275, Loss: 0.6749\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy: 0.6656, Validation Accuracy: 0.6449, Loss: 0.5348\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy: 0.6617, Validation Accuracy: 0.6669, Loss: 0.5025\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy: 0.7126, Validation Accuracy: 0.7060, Loss: 0.3989\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy: 0.7891, Validation Accuracy: 0.7525, Loss: 0.3397\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy: 0.8094, Validation Accuracy: 0.7919, Loss: 0.3237\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy: 0.8491, Validation Accuracy: 0.8256, Loss: 0.2686\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy: 0.8315, Validation Accuracy: 0.8359, Loss: 0.2264\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy: 0.8555, Validation Accuracy: 0.8466, Loss: 0.2217\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy: 0.8853, Validation Accuracy: 0.8423, Loss: 0.1685\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy: 0.9025, Validation Accuracy: 0.8494, Loss: 0.1559\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy: 0.9059, Validation Accuracy: 0.8612, Loss: 0.1632\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy: 0.9149, Validation Accuracy: 0.8885, Loss: 0.1201\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy: 0.9252, Validation Accuracy: 0.8903, Loss: 0.1207\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy: 0.9199, Validation Accuracy: 0.9038, Loss: 0.1338\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy: 0.9330, Validation Accuracy: 0.9194, Loss: 0.1005\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy: 0.9334, Validation Accuracy: 0.9119, Loss: 0.0946\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy: 0.9270, Validation Accuracy: 0.9130, Loss: 0.1030\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy: 0.9445, Validation Accuracy: 0.9197, Loss: 0.0736\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy: 0.9457, Validation Accuracy: 0.9382, Loss: 0.0681\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy: 0.9559, Validation Accuracy: 0.9411, Loss: 0.0746\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy: 0.9597, Validation Accuracy: 0.9347, Loss: 0.0552\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy: 0.9557, Validation Accuracy: 0.9318, Loss: 0.0658\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9403, Loss: 0.0647\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9343, Loss: 0.0596\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy: 0.9598, Validation Accuracy: 0.9386, Loss: 0.0529\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy: 0.9637, Validation Accuracy: 0.9503, Loss: 0.0542\n",
      "Epoch  10 Batch  300/1077 - Train Accuracy: 0.9650, Validation Accuracy: 0.9503, Loss: 0.0382\n",
      "Epoch  10 Batch  600/1077 - Train Accuracy: 0.9661, Validation Accuracy: 0.9542, Loss: 0.0426\n",
      "Epoch  10 Batch  900/1077 - Train Accuracy: 0.9539, Validation Accuracy: 0.9599, Loss: 0.0416\n",
      "Epoch  11 Batch  300/1077 - Train Accuracy: 0.9679, Validation Accuracy: 0.9609, Loss: 0.0456\n",
      "Epoch  11 Batch  600/1077 - Train Accuracy: 0.9714, Validation Accuracy: 0.9599, Loss: 0.0310\n",
      "Epoch  11 Batch  900/1077 - Train Accuracy: 0.9742, Validation Accuracy: 0.9698, Loss: 0.0344\n",
      "Epoch  12 Batch  300/1077 - Train Accuracy: 0.9778, Validation Accuracy: 0.9588, Loss: 0.0306\n",
      "Epoch  12 Batch  600/1077 - Train Accuracy: 0.9665, Validation Accuracy: 0.9691, Loss: 0.0341\n",
      "Epoch  12 Batch  900/1077 - Train Accuracy: 0.9836, Validation Accuracy: 0.9638, Loss: 0.0374\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the model, we want to avoid overfitting. Both overfitting and underfitting will reduce the performance. Overfitting happens when a model learns the detail and noise in the training data which will pock up the noise or random fluctuations which our concept do not apply to them. To find out if it is overfitting, we have cross validation.\n",
    "At the moment this model has an accuracy of ~98% on the training set and ~96% on the validation set. This means that we can expect our model to perform with ~96% accuracy on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [59, 112, 228, 30, 102, 161, 65]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [135, 357, 22, 103, 236, 33, 138, 125, 1]\n",
      "  French Words: il a pas un vieux camion jaune . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For here, the meaning of French Words we have is He doesn't have an old yellow truck. Different meaning(a word will change the whole meaning but from word to word, not bad if we get rid of the meaning just see the word prediction, actually, not bad), since we only have two small datasets and only run 10 minutes for training if we want to improve the result, we can just use the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building Encoder- Decoder, we need to utilize RNN in both Encoder and Decoder. Take an input sequence and each word from the input sequence is associated to a vector so we create a lookup table. And run an LSTM over the sequence of vectors and store the last hidden state output. After this, we have a vector that captures the meaning of the input sequence, we can use it to generate the target sequence word by word. Feed to another LSTM cell: the vector as hidden state. Then apply some functions to get another vector and has the same size as the vocabulary. Then apply softmax to creat a probability vector which will help us to determine the final output.\n",
    "\n",
    "Also, there is an interesting issue, for example two English words equivalent to one French word. I also find an interesting topic about Attention and Beam Search to solve this question.\n",
    "\n",
    "I really learn a lot from this. It is not a super difficult even if some parts I am still considering. However, it is a good start for NLP. How to preprocess data, why we should tokenize sentence, how RNN works, what models include in this seq2seq model, how to train the model and etc. I think I figure it out. It is a very interesting topic.\n",
    "\n",
    "We try long short- term memory (LSTM). LSTM has feedback connections that make it a \"general purpose computer\" (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). for RNN. In the same year there was two methods of RNN appeared- LSTM and GRU. We can also try GRU and then compare these two models and then evaluate these models.\n",
    "\n",
    "For the future work, I will try to translate Chinese to English. Or it can also the take task of automatically converting one natural language into another, producing fluent text in the output language. Also, since we have store the data as check point we can simply use them in the future."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
