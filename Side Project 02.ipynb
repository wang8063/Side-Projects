{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Topic Extraction\n",
    "</h1>\n",
    "<div align=\"center\">\n",
    "<font size=3><b>\n",
    "<br>Ruobing Wang\n",
    "<br>March 8, 2019\n",
    "<br></font></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Objectives:\n",
    "\n",
    "We will use the dataset from sklearn which is related with The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The dataset includes approximately 20,000 newsgroup documents, across 20 different newsgroups. We want to extract the 10 main topics from the dataset by using NMF and LDA methods. \n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "- Topic extraction analysis\n",
    "- Explain techniques and concepts used in code example\n",
    "- Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "Reference:\n",
    "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is related with The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    " <br/>\n",
    "\n",
    "\n",
    "What is inside:\n",
    "It has 20 different news organizations and each mpas the different themes. Some of them are very closely to each other and others are highly unrelated. \n",
    "\n",
    "The followings are the list of the 20 newsgroups:\n",
    "\n",
    "-----------------------------\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\t\n",
    "\n",
    "------------------------------\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "\n",
    "-----------------------------\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "\n",
    "-----------------------------\n",
    "misc.forsale\t\n",
    "\n",
    "-----------------------------\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "\n",
    "-----------------------------\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian\n",
    "\n",
    "\n",
    "\n",
    "- The background information of the data can be found at https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#newsgroups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.167s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly summary of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\", shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\", shuffle = True)\n",
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking from the dataset, it has a few broad topics like:\n",
    "\n",
    "- Science\n",
    "- Politics\n",
    "- Sports\n",
    "- Religion\n",
    "- Technology etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Load the 20 newsgroups dataset and vectorize it. We use a few heuristics to filter out useless terms early on: the posts are stripped of headers, footers and quoted replies, and common English words, words occurring in only one document or in at least 95% of the documents are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of words matrix, we use Scikit learn. A tf- idf can be applied to the bag of words matrix that NMF must process with the TfidfVectorizer. LDA, using a probabilistic graphical model which only requires raw counts, so a CountVectorizer is used. n features meands the bag of words matrix is restricted to the top 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.317s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.246s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the tf-idf features (NMF method):\n",
    "\n",
    "tf-idf(term frequencyâ€“inverse document frequency) intends to reflect how important is to documents. And for the TfidfVectorizer function, at first, we want the times of words appear not too much or too rare. It is because that for the words appear too much, the word maybe, such like 'I' or 'he', which is not meaningful for us. So he set the min_df = 2 and max_df = 0.95 which means we set 95% as our boundary and I believe the max_df has the similar function with srop_words.\n",
    "\n",
    "Also, for the features:  only consider the top 1000 features ordered by term frequency across the documents.\n",
    "\n",
    "And for inverse document frequency factor, which will diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.268s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a linear algeabreic model, factors high dimensional vectors into a low dimensionality representation. It takes advantage of the fact that the vectors are non negative. It forces the coefficients be npn negative.\n",
    "\n",
    "The idea of NMF: V=WH (W weight matrix, H feature matrix, V original matrix), by computing two different matrices from the original matrix to extract weights and features v will contain the weight of words and the columns will be the documents. An algorithm that belongs to an unsupervised learning, where the constraint is that all elements in W and H are greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the differences between the topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frobenius norm:\n",
    "\n",
    "$${\\displaystyle \\|A\\|_{\\rm {F}}={\\sqrt {\\sum _{i=1}^{m}\\sum _{j=1}^{n}|a_{ij}|^{2}}}={\\sqrt {\\operatorname {trace} \\left(A^{*}A\\right)}}={\\sqrt {\\sum _{i=1}^{\\min\\{m,n\\}}\\sigma _{i}^{2}(A)}},} $$\n",
    "\n",
    "$$||A||_F^2 = \\sum_{i,j} A_{ij}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the NMF function, n_compinents are the topics, here we have 10 topics and we set the random seed (random_state) equals to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the default of the loss function which is Frobenius norm. We are finding the shortest distance between the feartures in a topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.234s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback- Leibler \n",
    "\n",
    "Here, we set the loss function is 'kullback-leibler', and we have the multiplicate- update.\n",
    "\n",
    "By using this method we are accumulating the distance between the sparse martrics(tfidf) and the matrix X which is the two non-negative matrices (W, H) whose product approximates the non- negative matrix.\n",
    "\n",
    "KL divergence(relative entropy) is also a non-negative asymmetric to evaluate the difference between 2 probalitity dustributions.\n",
    "\n",
    "It is commonly used to measure loss in machine learning and measure of how one probability distribution is different from a second reference probability distribution. \n",
    "\n",
    "$${\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {P(x)}{Q(x)}}\\right).}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.029s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: people just like time don say really know way things make think right said did want ve probably work years\n",
      "Topic #1: windows thanks using help need hi work know use looking mail software does used pc video available running info advance\n",
      "Topic #2: god does true read know say believe subject says religion mean question point jesus people book christian mind understand matter\n",
      "Topic #3: thanks know like interested mail just want new send edu list does bike thing email reply post wondering hear heard\n",
      "Topic #4: time new 10 year sale old offer 20 16 15 great 30 weeks good test model condition 11 14 power\n",
      "Topic #5: use number com government new university data states information talk phone right including security provide control following long used research\n",
      "Topic #6: edu try file soon remember problem com program hope mike space article wrong library short include win little couldn sun\n",
      "Topic #7: year world team game play won win games season maybe case second does did series playing nhl fact said points\n",
      "Topic #8: think don drive need hard make people mac read going pretty try sure order means trying apple case bit drives\n",
      "Topic #9: just good use way got like ll doesn want sure don doing thought does wrong right better make stuff speed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the tf features (LDA method):\n",
    "\n",
    "tf(term frequency) also intends to reflect how important is to documents. And for the CountVectorizer function, the parameters are basically have the same meaning.\n",
    "However, \n",
    "since it does not have the idf, which means will not weight down the frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.269s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA:\n",
    "\n",
    "LDA(latent Dirichlet allocation) with term frequency. Basically, it can let obeservations to be explained by unobservered groups. And explain why some of the parts of the data are similar. The model can be estimated by 2 matrics, which discribes the probability of a selected part.\n",
    "\n",
    "LDA is an unsupervised machine learning model which can take a document as input and find topics as output. It is used to classify text in a document to a particular topic. It uses two probability values words topics and topics documents. \n",
    "\n",
    "Because LDA requires data in the form of integer counts, modifying feature values using TF-IDF and combine them will not fit well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 2.950s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 componets means we have 10 topics and since the max iterations equals to 5 which means we repeat 5 times. \n",
    "\n",
    "For the learning_method, we have 2 method: Batch or Online.\n",
    "\n",
    "Batch: the only thing we care about is the current iteration.\n",
    "Online: Rather than rewrite, we mutating stuffs, thus the previous steps matter.\n",
    "\n",
    "For the learning_offset, basically, it will let the iteration starts gently and graduately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people just like time don say really know way things\n",
      "Topic 1:\n",
      "windows thanks using help need hi work know use looking\n",
      "Topic 2:\n",
      "god does true read know say believe subject says religion\n",
      "Topic 3:\n",
      "thanks know like interested mail just want new send edu\n",
      "Topic 4:\n",
      "time new 10 year sale old offer 20 16 15\n",
      "Topic 5:\n",
      "use number com government new university data states information talk\n",
      "Topic 6:\n",
      "edu try file soon remember problem com program hope mike\n",
      "Topic 7:\n",
      "year world team game play won win games season maybe\n",
      "Topic 8:\n",
      "think don drive need hard make people mac read going\n",
      "Topic 9:\n",
      "just good use way got like ll doesn want sure\n",
      "Topic 0:\n",
      "edu com mail send graphics ftp pub available contact university\n",
      "Topic 1:\n",
      "don like just know think ve way use right good\n",
      "Topic 2:\n",
      "christian think atheism faith pittsburgh new bible radio games alt\n",
      "Topic 3:\n",
      "drive disk windows thanks use card drives hard version pc\n",
      "Topic 4:\n",
      "hiv health aids disease april medical care research 1993 light\n",
      "Topic 5:\n",
      "god people does just good don jesus say israel way\n",
      "Topic 6:\n",
      "55 10 11 18 15 team game 19 period play\n",
      "Topic 7:\n",
      "car year just cars new engine like bike good oil\n",
      "Topic 8:\n",
      "people said did just didn know time like went think\n",
      "Topic 9:\n",
      "key space law government public use encryption earth section security\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\"% (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived topics from NMF and LDA are displayed above, LDA for the 20 newsgroup dataset has noise data for example topic 6, and some topics are hard to interpret. From the result, I would like to say, the NMF have more meaningful topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, I think we cannot get the certain answer. Since here, we only use 10 topics (20 in total for the original), we will not any specific metric to evaluate how well the model is performed because they have the similar groups.\n",
    "\n",
    "The structure of the resulting matrices returned by both NMF and LDA is the same which is great and allows for the python method so we can display the top words in a topic.\n",
    "\n",
    "Also, we may can from some keywords such like Christian in the topic 2 which we show above to refer this topic should from religion and Christian topic. For the future, I will try to utilize pyLDAvis to visualize model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
