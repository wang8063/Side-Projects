{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\"> Textual classification\n",
    "<div align=\"center\">\n",
    "<font size=3><b>\n",
    "<br>Ruobing Wang\n",
    "<br>March 15, 2019\n",
    "<br></font></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Text classification is the task of automatically classify the text documents into one or more defined categories. It can be used in Detection of spam and non spam emails etc. Text classifiers can be used to organize and categorize pretty. For here, we continue using the dataset of 20 newsgroups which contains approximately 20,000 newsgroup documents, across 20 different newsgroups. Because we need the documents and a fixed set of classes as our input and the output will be the predict class from the sets. For the classification methods, we can use both Hand- coded rules or supervised machine learning. For here, I will use the Naive Bayes and Support Vector Machine as classifier to get our output and then use Grid search to improve the accuracy.  \n",
    "### Objectives\n",
    "\n",
    "- Textual pipeline\n",
    " - Textual classification\n",
    "\t\t- Naive Bayes \n",
    "\t\t- Support Vector Machine\n",
    "- Grid search\n",
    "\n",
    "### Reference \n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "https://www.quora.com/What-are-popular-text-classification-algorithms-in-commercial-use-and-how-are-they-used\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is related with The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    " <br/>\n",
    "\n",
    "\n",
    "What is inside:\n",
    "It has 20 different news organizations and each mpas the different themes. Some of them are very closely to each other and others are highly unrelated. \n",
    "\n",
    "The followings are the list of the 20 newsgroups:\n",
    "\n",
    "-----------------------------\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\t\n",
    "\n",
    "------------------------------\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "\n",
    "-----------------------------\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "\n",
    "-----------------------------\n",
    "misc.forsale\t\n",
    "\n",
    "-----------------------------\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "\n",
    "-----------------------------\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian\n",
    "\n",
    "\n",
    "\n",
    "- The background information of the data can be found at https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#newsgroups\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background of the Dataset:\n",
    "\n",
    "This dataset is a collection of approximately 20,000 newsgroup documents, nearly evenly across 20 different newsgroups. All of the information are collected by Ken Lang for his paper. \n",
    "\n",
    "This data has become a popular data set fir experiments for such as text classification and text clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the fetch_20newsgroups, includes two parts, one for training (or development) and the other one for testing (or for performance evaluation). We will just use the data for training. And, in conclusion, in this step, we can choose the data which is matched in the categories and trained after we randomly shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.335s.\n",
      " \n",
      "Summary of the dataset:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from time import time\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:2000]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\" \")\n",
    "print(\"Summary of the dataset:\")\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\", shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\", shuffle = True)\n",
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, in the 20 available categories, in order to make the execution times shorter, we choose 4 topics of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the list of files matching those 4 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have the object which can be accessed as dict or object in python for convenience. Now, the object includes the following categories.\n",
    "\n",
    "Then, lets see the summary of the datasets we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the requested category names which is good and we can use the data attribute also names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first line of the first loaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "\n",
    "\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the supervised learning algorithms, it requires a category label for each document, for here, we have the index of the category name and we store this as the integer. These integers are stored in the target attribute. Every number(category) is the name of the newsgroup, we can see how they mapped from the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we shuffled randomly in the fetch_20newsgroups function which is useful for us since we wish to select only a subset of samples to train a model and get the results quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, it has two parts: one for training (or development) and the other one for testing (or for performance evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we need to turn the text content into numerical feature vectors since we want to perform machine learning on text documents for performing machine learning on text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words is a way of extracting features from text for use in modeling. It is a representation of text that describes the occurrence of words within a document. We do not need to consider of the order and structure of words just concern whether known words occur in the document or not.\n",
    " \n",
    "It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text with scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform:\n",
    "\n",
    "- Fit: apply the algorithm to the data and find a proper algorithm and will the algorithm, \n",
    "    \n",
    "- Transform, only the count vectorizer and tfidf_transformer have transform function, basically it will creat a copy and mutate the copy and return a new copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using CountVectorizer, which includes Text preprocessing, tokenizing and filtering of stopwords (split the sentence to words). We can build a dictionary of features and transform documents to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the CountVectorizer fitted, it will built a dictionary to provide the feature indices. For here, we have the unicode of the 'algorithm' which is 4690 for here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From occurrences to frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the occurrence, it will have problems such that the longer documents have higher average count values than shorter and there are some uninformative words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency\n",
    "To avoid the first problem, we can divide the number of the occurrences of each word in a document by total number of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency times Inverse Document Frequency\n",
    "To deal with the second problem, we can downscale the weights of each word and why we do in this way is less informative words will only occur in a smaller portion of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the conclusion:\n",
    "\n",
    "- TF\n",
    "    - Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.\n",
    "\n",
    "- TF-IDF\n",
    "    - we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we fit our estimator to the data and then transform the count- matrix to a tf-idf presentation or we can also combine these two steps by using .fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised machine learning, we can use any kind of classifier like:\n",
    "- Naive Bayes\n",
    "- Logistic regression\n",
    "- Support vector machines\n",
    "- k-Nearest Neighbors\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier is another method to extract features. Multinomial distribution here, we use the counts from tf-idf from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class , where y is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability  of feature $P(x_i \\mid y)$ appearing in a sample belonging to class .\n",
    "\n",
    "The parameters $\\theta_{y}$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature  appears in a sample of class $T$ in the training set , and  is the total count of all features for class .\n",
    "\n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding let's see this example:\n",
    "\n",
    "I have a sentence \"A very close game\" a training set of 5 sentences like \"A great game\" \"The election was over\" \"Very clean match\" \"A clean but forgettable game\" \"A close election\" and the category (Sports or Not sports). THe Naive Bayes classifier we need to give it a naive assumption like the every feature is independent to others. We calculate the probability P(Sports| A very close game) etc. P(category| sentence).\n",
    "\n",
    "Then we extract features of text (what we did before). Then convert the the probability to let us easy to calculate use the principle of conditional probability which is \n",
    "$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$$\n",
    "we want to find which category has a higher probability. Thus we can just discard the divisor simply we can compare the numerator. But we have another problem which is to count the occurrence of the sentence in the Sports category. For this example since it is not in the training model, the probability is zero.\n",
    "To fix that, we need to be naive to accept the assumption that every word is independent of one another. So we can look at the individual words in the sentence instead of the whole sentence.\n",
    "$$P(a very close game) = P(a)*P(very)*P(close)*P(game)$$\n",
    "Then just to see which one have a higher probability when face the condition Sports or Not Sports. But since sometimes we do not have the word exist in the dataset, which the prob will equal to zero and lead to the whole sentence's probability equals to zero. To fix that, we can use Laplace smoothing. The function we talk above. For simply set the parameters we can let the division always greater than 1 and let the probability never be zero.\n",
    "\n",
    "So, we can see from this, our category will be the 4 categories and sentences are from the 20 newsgroup dataset. Let's say I have a sentence \"God is love\" in Christian, So the probability will be P(Christian|\"God is love\") and we can calculate the probability of $P(God is love) = P(God)*P(is)*P(love)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's see the parameters now:\n",
    "\n",
    "At first, we calculate the frequencies by tfidf and fit the estimator to data and transform to a tfidf representation(X_train_tfidf)\n",
    "\n",
    "Then we fit the tfidf representation form and the 4 categories(atheism, graphics, med and Christian) in the Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We using the MultinomialNB algorithm to fit the data we used in tf-idf as our feature counts to the integer id of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use a list example of sentence to predict the categories where it may from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use vectorizer => transformer => classifier three steps to reach our goal, we can just simplify that with building a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "\n",
    "*-----------------------------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline will gather the 3 or (several) steps to one step. Instead step by step, store value in the variable, we can use a single step to reach our purpose. For here after we Vectorizer, transform to tfidf representation of count- matrix, and classfier, we can directly train the model use only one command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculate the accuracy, we see we only have 83.5% accuracy. Let's try the second text classification algorithm- using SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine with stochastic gradient discent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are the set of supervised learning methods used for classification, regression and outliers detection. It's advantages are effective in high dimensional spaces or the number of dimensions is greater than the number of samples. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of integer features we used before, here, this works with array of floating point values as the features. Here, we use the parameter(default) to let the model fits a linear support vector machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM's parameter: SGD classifier is Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "loss : The loss function to be used which gives a linear SVM.\n",
    "\n",
    "alpha :Constant that multiplies the regularization term. Defaults to 0.0001\n",
    "    Also used to compute learning_rate when set to 'optimal'.\n",
    "penalty: The penalty 'l2' is the standard regularizer for linear SVM models. \n",
    "    \n",
    "And we set iteration 5 and random 42 times.\n",
    "\n",
    "And then we fit both the data of size of 2257 we load before and the 4 categories to this model(pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved 91.3% accuracy using the SVM. scikit-learn provides further utilities for more detailed performance analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "             micro avg       0.91      0.91      0.91      1502\n",
      "             macro avg       0.92      0.91      0.91      1502\n",
      "          weighted avg       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix is a table that is often used to describe the performance of a classification model. The columns are predicted and rows are actual. Combine these blocks we will have TP(True positive is Predict = Actual), TN(true negative), FP(false positive), FN(False negative).\n",
    "\n",
    "The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions. For example, the number of when predicted sentences in atheism and actual sentences in the atheism are same is 258. Similarly, the number of predict and actual sentences in graphics are equal is 379. Med is 355 and christian is 379. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: $\\frac{tp}{Predicted}$, for example, for atheism it would be $\\frac{258}{258+4+5+5}=0.95$ which the 258+4+5+5 are from the column of atheism. It measures when it predicts yes, how often is it correct.\n",
    "\n",
    "Recall:is $\\frac{TP}{TP+FP}$, for example, for atheism it would be: $\\frac{258}{258+11+15+35}=0.81$ 258+11+15+35 are from rows. Recall helps when the cost of false negatives is high. \n",
    "\n",
    "F1-score is an overall measure of a model’s accuracy that combines precision and recall, in that weird way that addition and multiplication just mix two ingredients to make a separate dish altogether. That is, a good F1 score means that you have low false positives and low false negatives, so you’re correctly identifying real threats and you are not disturbed by false alarms. An F1 score is considered perfect when it’s 1, while the model is a total failure when it’s 0. $F1 = 2*\\frac{precision*recall}{precision + recall}$ for atheism: F1= 1539/1760 = 0.81\n",
    "\n",
    "Support number of total cases for category, for example, atheism has 319"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To let the result more direct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1fb3b208>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = metrics.confusion_matrix(twenty_test.target, predicted)\n",
    "sns.heatmap(m,cmap=\"PuBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart, we can directly see the most confused newsgroups are atheism and Christianity are more often confused for one another than with computer graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the classifiers will have various parameters which can be tuned to reach the best performance. Such like use_idf in the TfidfTransformer. Or penalty parameter alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the exhaustive search of the best parameters on a grid of possible values. In the ngram, we try unigram(each words), and bigrams. With or not idf, and we have the penalty (like offset) parameter of 0.1 or 0.001. By the different possible combinations, instead of tweak the parameters of multiple steps, we can try out all classifiers. And at last, we smooth the alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this method will cost a lot, multiple core cpu will let this becomes easier to do, since it will deal with stuffs parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give n_jobs =1 since for that it can detect how many cores are installed and use all of them to increase the speed by trying put these parameter combinations in parallel since this search can be expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf =GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's perform the search on a smaller subset to speed up the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object's best score and best params attributes store the best mean and the parameters setting corresponding to the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9151349867929058"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, By using grid search, we can find the best parameter of Naive Bayes(for this example, but we can also utilize it in the other model). Also, if we change the parameter of NB Classifier, and compare the result we get above, we can see the accuracy will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, in this lab, I learned 3 things (2 methods and an extremely useful tool \"GridSerachCV\" from Scikit)of the Textual classification. \n",
    "\n",
    "I learned the concepts like bag of words, TFIDF and the algorithms NB and SVM. From the result after we change the parameter, we can get a similar result. \n",
    "\n",
    "The first model. It has 3 steps. Loading Dataset, extract features(we can use bags of words, tokenizing text, and tf-idf) After we have the features, we can train the classifier. To simplify the steps, we can build the pipeline to make vectorizer => transformer => classifier easier to work with. \n",
    "\n",
    "And then, we calculate the accuracy of prediction. See that we can improve by SVM, a set of supervised learning methods used for classification, regression and outliers detection. \n",
    "\n",
    "The third one is grid search. Instead of tweaking the parameters of the various components of the chain, we run an exhaustive search of the best parameters on a grid of possible values.\n",
    "\n",
    "For improving the the Naive Bayes classifier, except using TF-IDF, we can also try removing stopwords, lemmatizing words and using n-grams.\n",
    "\n",
    "As the conclusion, the SVM is a better text classification algorithms. It has the higher accuracy before we gridsearch, and easiest to take advantage of. However, grid search is expensive to work with but we can plug the best parameter into the model so that we can improve the accuracy. For my recommendations are using CNN text classifier in TensorFlow and RNN Sentence classification tutorial in Keras to see whether we can improve the accuracy and how much it costs.\n",
    "\n",
    "Businesses are turning to text classification for structuring text in a fast and cost-efficient way to enhance decision-making and automate processes. And now it can use in such like social media monitoring or customer service and etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
